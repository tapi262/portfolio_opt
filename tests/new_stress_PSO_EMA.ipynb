{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a952ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(base_dir, sample_n=50):\n",
    "    all_data = []\n",
    "    tickers_seen = set()\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        np.random.shuffle(files)\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # ticker z nazwy pliku\n",
    "                    ticker = os.path.splitext(file)[0]\n",
    "                    if ticker in tickers_seen:\n",
    "                        continue\n",
    "\n",
    "                    # wczytanie danych zgodnie z nowym formatem\n",
    "                    df = pd.read_csv(path, skiprows=2, header=None,\n",
    "                                     names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n",
    "                                     usecols=[\"Date\", \"Close\"])\n",
    "                    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "                    df = df.dropna(subset=[\"Date\", \"Close\"])\n",
    "\n",
    "                    if df.shape[0] > 100:\n",
    "                        df[\"TICKER\"] = ticker\n",
    "                        all_data.append(df)\n",
    "                        tickers_seen.add(ticker)\n",
    "\n",
    "                        if len(tickers_seen) >= sample_n:\n",
    "                            break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if len(tickers_seen) >= sample_n:\n",
    "            break\n",
    "\n",
    "    if not all_data:\n",
    "        raise Exception(\"Brak danych\")\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "def prepare_pivot(df, start_year=2020, end_year=2025, max_nan_percent=5.0):\n",
    "    df = df[(df[\"Date\"].dt.year >= start_year) & (df[\"Date\"].dt.year <= end_year)]\n",
    "    pivot = df.pivot(index=\"Date\", columns=\"TICKER\", values=\"Close\").sort_index()\n",
    "    pivot = pivot[pivot.index.weekday < 5]  # usuÅ„ weekendy\n",
    "    nans = pivot.isna().sum() / len(pivot) * 100\n",
    "    good_tickers = nans[nans <= max_nan_percent].index.tolist()\n",
    "    pivot = pivot[good_tickers].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cc44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(price_df: pd.DataFrame, window: int = 20):\n",
    "    returns = price_df.pct_change().dropna()\n",
    "    momentum = returns.tail(window).mean()\n",
    "    volatility = returns.tail(window).std()\n",
    "    sharpe = momentum / volatility\n",
    "    cum_returns = (1 + returns.tail(window)).cumprod()\n",
    "    peak = cum_returns.cummax()\n",
    "    drawdown = (cum_returns - peak) / peak\n",
    "    mdd = drawdown.min()\n",
    "    corr_matrix = returns.tail(window).corr()\n",
    "    total_corr = corr_matrix.sum() - 1\n",
    "    features = pd.DataFrame({\n",
    "        'MOM': momentum,\n",
    "        'VOL': volatility,\n",
    "        'SHARPE': sharpe,\n",
    "        'MDD': mdd,\n",
    "        'CORR': total_corr\n",
    "    })\n",
    "    return features.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd65400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso_fitness_advanced(weights, features, lambda_mom=0.0, lambda_vol=0.4, lambda_mdd=0.3, lambda_sharpe=0.2, lambda_corr=0.1):\n",
    "    port_mom = np.sum(weights * features[\"MOM\"])\n",
    "    port_vol = np.sqrt(np.sum((weights * features[\"VOL\"])**2))\n",
    "    port_sharpe = port_mom / port_vol if port_vol != 0 else 0\n",
    "    port_mdd = np.sum(weights * features[\"MDD\"])\n",
    "    port_corr = np.sum(weights * features[\"CORR\"])\n",
    "    fitness = (\n",
    "        - lambda_mom * port_mom +\n",
    "        lambda_vol * port_vol +\n",
    "        lambda_mdd * abs(port_mdd) -\n",
    "        lambda_sharpe * port_sharpe +\n",
    "        lambda_corr * port_corr\n",
    "    )\n",
    "    return fitness\n",
    "\n",
    "def pso_optimize_advanced(features: pd.DataFrame, n_particles=100, n_iterations=300, max_weight=0.2,\n",
    "                          lambda_mom=0.0, lambda_vol=0.4, lambda_mdd=0.3, lambda_sharpe=0.2, lambda_corr=0.1):\n",
    "    n_assets = len(features)\n",
    "    particles = np.random.dirichlet(np.ones(n_assets), size=n_particles)\n",
    "    velocities = np.zeros_like(particles)\n",
    "    personal_best = particles.copy()\n",
    "    personal_scores = np.array([\n",
    "        pso_fitness_advanced(p, features, lambda_mom, lambda_vol, lambda_mdd, lambda_sharpe, lambda_corr)\n",
    "        for p in particles\n",
    "    ])\n",
    "    global_best_idx = np.argmin(personal_scores)\n",
    "    global_best = personal_best[global_best_idx].copy()\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        for i in range(n_particles):\n",
    "            r1, r2 = np.random.rand(n_assets), np.random.rand(n_assets)\n",
    "            velocities[i] = 0.5 * velocities[i] + 1.4 * r1 * (personal_best[i] - particles[i]) + 1.4 * r2 * (global_best - particles[i])\n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], 0, max_weight)\n",
    "            particles[i] /= particles[i].sum() if particles[i].sum() > 0 else 1\n",
    "            score = pso_fitness_advanced(particles[i], features, lambda_mom, lambda_vol, lambda_mdd, lambda_sharpe, lambda_corr)\n",
    "            if score < personal_scores[i]:\n",
    "                personal_best[i] = particles[i].copy()\n",
    "                personal_scores[i] = score\n",
    "                if score < personal_scores[global_best_idx]:\n",
    "                    global_best = particles[i].copy()\n",
    "                    global_best_idx = i\n",
    "\n",
    "    return global_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realistic_pso_ema_advanced(pivoted_prices: pd.DataFrame,\n",
    "                                feature_window: int = 20,\n",
    "                                rebalance_period: int = 5,\n",
    "                                n_particles: int = 100,\n",
    "                                n_iterations: int = 300,\n",
    "                                max_weight: float = 0.2,\n",
    "                                lambda_mom: float = 0.0,\n",
    "                                lambda_vol: float = 0.4,\n",
    "                                lambda_mdd: float = 0.3,\n",
    "                                lambda_sharpe: float = 0.2,\n",
    "                                lambda_corr: float = 0.1,\n",
    "                                fee_rate: float = 0.001,\n",
    "                                ema_span: int = 20):  \n",
    "\n",
    "    smoothed_prices = pivoted_prices.ewm(span=ema_span, adjust=False).mean()\n",
    "\n",
    "    returns = smoothed_prices.pct_change().dropna()\n",
    "    returns_test = returns[returns.index.year == 2025]\n",
    "    dates = returns_test.index\n",
    "\n",
    "    capital = 1.0\n",
    "    capital_timeline = []\n",
    "    weights_log = []\n",
    "    shares = None\n",
    "    prev_weights = pd.Series(0.0, index=pivoted_prices.columns)\n",
    "\n",
    "    for start in range(0, len(dates), rebalance_period):\n",
    "        end = min(start + rebalance_period, len(dates))\n",
    "        rebalance_date = dates[start]\n",
    "        period_dates = dates[start:end]\n",
    "        current_prices = smoothed_prices.loc[rebalance_date]\n",
    "\n",
    "        price_history = smoothed_prices.loc[:rebalance_date].tail(feature_window + 1)\n",
    "        features = extract_features(price_history, window=feature_window)\n",
    "\n",
    "        new_weights = pso_optimize_advanced(\n",
    "            features,\n",
    "            n_particles=n_particles,\n",
    "            n_iterations=n_iterations,\n",
    "            max_weight=max_weight,\n",
    "            lambda_mom=lambda_mom,\n",
    "            lambda_vol=lambda_vol,\n",
    "            lambda_mdd=lambda_mdd,\n",
    "            lambda_sharpe=lambda_sharpe,\n",
    "            lambda_corr=lambda_corr\n",
    "        )\n",
    "\n",
    "        total_value = (shares * current_prices).sum() if shares is not None else capital\n",
    "        cost = fee_rate * total_value * np.sum(np.abs(new_weights - prev_weights))\n",
    "        total_value_after_cost = total_value - cost\n",
    "        capital = total_value_after_cost\n",
    "\n",
    "        shares = (new_weights * total_value_after_cost) / current_prices\n",
    "        prev_weights = new_weights\n",
    "\n",
    "        for date in period_dates:\n",
    "            prices_today = smoothed_prices.loc[date]\n",
    "            portfolio_value = np.sum(shares * prices_today)\n",
    "            capital_timeline.append((date, portfolio_value))\n",
    "\n",
    "        for ticker, w in zip(pivoted_prices.columns, new_weights):\n",
    "            weights_log.append({\n",
    "                \"Date\": rebalance_date,\n",
    "                \"Ticker\": ticker,\n",
    "                \"Weight\": w\n",
    "            })\n",
    "\n",
    "    capital_series = pd.Series(dict(capital_timeline))\n",
    "    weights_df = pd.DataFrame(weights_log)\n",
    "    return capital_series, weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Basia\\AppData\\Local\\Temp\\ipykernel_19540\\1853472970.py:54: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  pivot = pivot[good_tickers].fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "base_dir = r\"C:\\Users\\Basia\\Do przejrzenia\\am_sem2\\mgr\\kody\\downloaded_data\"\n",
    "df_all = load_data(base_dir, sample_n=771)\n",
    "pivoted = prepare_pivot(df_all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94130f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling stress test iterations - PSO EMA:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [1:59:08<5:11:06, 266.66s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "n_iterations = 100\n",
    "path_length = 22\n",
    "block_size = 5\n",
    "output_dir = \"rolling_results_ema/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def generate_stress_data(pivoted_df):\n",
    "    returns = pivoted_df.pct_change().dropna()\n",
    "    returns[\"SCENARIO\"] = \"Dummy\"\n",
    "    return returns\n",
    "\n",
    "\n",
    "def generate_bootstrap_stress_paths(returns_df, path_length, n_paths, block_size):\n",
    "    returns_df = returns_df.drop(columns=[\"SCENARIO\"], errors=\"ignore\")\n",
    "    returns_df = returns_df.dropna(axis=1)\n",
    "\n",
    "    block_starts = np.arange(len(returns_df) - block_size + 1)\n",
    "    paths = []\n",
    "\n",
    "    for _ in range(n_paths):\n",
    "        idx = np.random.choice(block_starts, size=(path_length // block_size + 1), replace=True)\n",
    "        sampled_blocks = [returns_df.iloc[i:i + block_size] for i in idx]\n",
    "        sampled_path = pd.concat(sampled_blocks).iloc[:path_length]\n",
    "        paths.append(sampled_path.reset_index(drop=True))\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "last_prices = pivoted.iloc[-1]\n",
    "historical_prices = pivoted.copy()\n",
    "\n",
    "stress_data_real = generate_stress_data(pivoted)\n",
    "sample_columns = pivoted.columns\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(n_iterations), desc=\"Rolling stress test iterations - PSO EMA\"):\n",
    "    stress_paths = generate_bootstrap_stress_paths(\n",
    "        stress_data_real[sample_columns],\n",
    "        path_length=path_length,\n",
    "        n_paths=1,\n",
    "        block_size=block_size\n",
    "    )\n",
    "    stress_path = stress_paths[0]\n",
    "\n",
    "    stress_prices = pd.DataFrame(\n",
    "        data=np.cumprod(1 + stress_path.values, axis=0) * last_prices.values,\n",
    "        columns=sample_columns,\n",
    "        index=pd.date_range(start=\"2025-05-01\", periods=path_length, freq=\"B\")\n",
    "    )\n",
    "\n",
    "    capital_series, weights_df = realistic_pso_ema_advanced(\n",
    "        pivoted_prices=stress_prices,\n",
    "        feature_window=20,\n",
    "        rebalance_period=5,\n",
    "        n_particles=200,\n",
    "        n_iterations=300,\n",
    "        max_weight=0.15,\n",
    "        lambda_mom=0.2,\n",
    "        lambda_vol=0.3,\n",
    "        lambda_mdd=0.3,\n",
    "        lambda_sharpe=0.3,\n",
    "        lambda_corr=0.3,\n",
    "        fee_rate=0.001\n",
    "    )\n",
    "\n",
    "    capital_series.to_csv(f\"{output_dir}/capital_iteration_{i+1}.csv\")\n",
    "    weights_df.to_csv(f\"{output_dir}/weights_iteration_{i+1}.csv\")\n",
    "    results.append(capital_series)\n",
    "\n",
    "capital_series_combined = pd.concat(results, axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "capital_series_combined.plot(figsize=(14, 7), title=\"KapitaÅ‚ w czasie - PSO EMA z okresami stresowymi\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
